{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOeZAoQhZ2ou"
      },
      "source": [
        "# Historical Weather Data Scrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfZEgnILZ6dW"
      },
      "source": [
        "This notebook has instructions for how to scrap historical weather data from https://www.timeanddate.com/weather/@3427761/historic?month=7&year=2010"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzHhMJf4aDGS"
      },
      "source": [
        "## 0. Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uc10dAOhZyGH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsN5j0hJVNfw"
      },
      "source": [
        "## 1. Function to Retrieve Weather Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F58OHvcFuv67"
      },
      "source": [
        "scrape_weather_data has 3 parameters:  \n",
        "1. Year\n",
        "2. Month\n",
        "3. Day\n",
        "\n",
        "The idea is to retrieve data from 2009 to 2023 from the website's table and then generate a dataframe and csv file with the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ypeFn-LVVTk8"
      },
      "outputs": [],
      "source": [
        "def scrape_weather_data(year, month, day):\n",
        "\n",
        "    # 1. set URL\n",
        "    url = f'https://www.timeanddate.com/weather/@3427761/historic?month={month}&year={year}&day={day}' # month, year, and day will be the parameters\n",
        "\n",
        "    # 2. request\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # 3. Find the table from the content of the response\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    table = soup.find(id='wt-his')\n",
        "\n",
        "    # 3.1 Validate the existence of the table\n",
        "    if table is not None:\n",
        "        # 4 .Extract data from each row\n",
        "        data_rows = []\n",
        "        rows = table.find_all('tr')\n",
        "\n",
        "        # 5. Now we loop through all the table's cells\n",
        "        for row in rows:\n",
        "            cells = row.find_all(['td', 'th'])\n",
        "            if len(cells) == 9:  # Check if the row has the expected number of cells\n",
        "                time = cells[0].text.strip()\n",
        "\n",
        "                # Skip rows that do not contain valid time information. This will make the code to not break if it didn't find information.\n",
        "                if ':' not in time:\n",
        "                    continue\n",
        "\n",
        "                # 6. Set variables with cell's content. We will use this for the dataframe columns\n",
        "                temperature = cells[2].text.strip()\n",
        "                weather = cells[3].text.strip()\n",
        "                wind_speed = cells[4].text.strip()\n",
        "                wind_direction = cells[5].text.strip()\n",
        "                humidity = cells[6].text.strip()\n",
        "                barometer = cells[7].text.strip()\n",
        "                visibility = cells[8].text.strip()\n",
        "\n",
        "                # 7. Split time string to extract day\n",
        "                date_str = f\"{year}-{month}-{day}\"\n",
        "                date = datetime.strptime(date_str, '%Y-%m-%d').date()\n",
        "\n",
        "                row_data = [date, time, temperature, weather, wind_speed, wind_direction, humidity, barometer, visibility]\n",
        "                data_rows.append(row_data)\n",
        "\n",
        "        # 8. Convert the list of rows into a DataFrame\n",
        "        df = pd.DataFrame(data_rows, columns=['Date', 'Time', 'Temperature', 'Weather', 'Wind Speed', 'Wind Direction', 'Humidity', 'Barometer', 'Visibility'])\n",
        "\n",
        "    else:\n",
        "        # If the table is not found\n",
        "        print(\"The table was not found on the page.\")\n",
        "        # Assign default values of zero to the dataframe.\n",
        "        df = pd.DataFrame({'Date': [0], 'Time': [0], 'Temperature': [0], 'Weather': [0], 'Wind Speed': [0], 'Wind Direction': [0], 'Humidity': [0], 'Barometer': [0], 'Visibility': [0]})\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqHn1IK1ukuZ"
      },
      "source": [
        "## 2. Call Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT-h_4fsubJq"
      },
      "source": [
        "For scrapping data from several years, you can set a for loop to iterate over years and months.  \n",
        "\n",
        "This is an example of how you can do it. In this case we retrieve data from only 2022 because we only had tide data from Tigre from 2022.  \n",
        "\n",
        "Note: this can take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a variable that will contain the list\n",
        "all_data = []\n",
        "\n",
        "# Loop from 2022 to 2023\n",
        "for year in range(2022, 2024):\n",
        "    for month in range(1, 13):\n",
        "\n",
        "        # Get the number of days in the month\n",
        "        days_in_month = 31 if month in [1, 3, 5, 7, 8, 10, 12] else 30 if month in [4, 6, 9, 11] else 28 if year % 4 != 0 or (year % 100 == 0 and year % 400 != 0) else 29\n",
        "\n",
        "        for day in range(1, days_in_month + 1):\n",
        "            df = scrape_weather_data(year, month, day)\n",
        "            all_data.append(df)\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Scraped data for {year}-{month}-{day}\")\n",
        "\n",
        "# Concatenate all dataframes into a single dataframe\n",
        "final_df = pd.concat(all_data, ignore_index=True)\n",
        "final_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCp_CYOtuS_l"
      },
      "source": [
        "## 3. Save CSV file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Don't forget to save your csv file once you scrapped your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_df.to_csv('tigre_weather_2022-2023.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
